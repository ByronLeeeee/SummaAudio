import time
import json
import configparser
import streamlit as st
import sys
import os

# Ensure the project root is in sys.path
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)

from scripts import ollama_scripts, openai_scripts
from scripts.utils import CONFIG_INI_PATH, setup_logger

logger = setup_logger("SettingsPage")

# Load existing configuration
config = configparser.ConfigParser()
config.read(CONFIG_INI_PATH, encoding="utf-8")


# Helper function to save config
def save_configuration(show_success_toast=True):
    """å°†å½“å‰é…ç½®å¯¹è±¡ä¿å­˜åˆ°INIæ–‡ä»¶ã€‚"""
    try:
        # Ensure config directory exists
        os.makedirs(os.path.dirname(CONFIG_INI_PATH), exist_ok=True)
        with open(CONFIG_INI_PATH, "w", encoding="utf-8") as configfile:
            config.write(configfile)
        if show_success_toast:
            st.toast("é…ç½®ä¿å­˜æˆåŠŸï¼", icon="âœ…")
        logger.info("Configuration saved successfully.")
        return True
    except Exception as e:
        st.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}", icon="âŒ")
        logger.error(f"Failed to save configuration: {e}")
        return False


st.header("âš™ï¸ åº”ç”¨è®¾ç½®")
st.caption("åœ¨æ­¤é¡µé¢é…ç½®åº”ç”¨çš„æ ¸å¿ƒå‚æ•°å’Œæ¨¡å‹è¿æ¥ä¿¡æ¯ã€‚")

tab_system, tab_modelscope, tab_ollama, tab_online_model = st.tabs( # Renamed tab_openai
    ["ğŸ–¥ï¸ ç³»ç»Ÿè®¾ç½®", "ğŸ—£ï¸ ModelScope", "ğŸ¦™ Ollama", "ğŸŒ åœ¨çº¿æ¨¡å‹ (OpenAIå…¼å®¹)"] # Changed tab label
)

with tab_system:
    st.subheader("LLM åç«¯é€‰æ‹©")
    current_llm_mode = config.get("SYSTEM", "llm_mode", fallback="Ollama")
    llm_mode_options = ["Ollama", "OpenAI"] # Keep 'OpenAI' here as internal identifier for type
    selected_llm_mode = st.radio(
        "é€‰æ‹©é»˜è®¤çš„LLMæœåŠ¡:",
        llm_mode_options,
        index=(
            llm_mode_options.index(current_llm_mode)
            if current_llm_mode in llm_mode_options
            else 0
        ),
        horizontal=True,
        help="é€‰æ‹©ç”¨äºæ–‡æœ¬ä¿®æ­£å’Œå½’çº³ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹åç«¯ã€‚é€‰æ‹© 'OpenAI' å°†ä½¿ç”¨ä¸‹æ–¹â€œåœ¨çº¿æ¨¡å‹â€æ ‡ç­¾é¡µé…ç½®çš„é»˜è®¤æ¨¡å‹ã€‚",
    )
    if st.button("ä¿å­˜ç³»ç»Ÿè®¾ç½®", key="save_system_settings", type="primary"):
        if "SYSTEM" not in config:
            config.add_section("SYSTEM")
        config["SYSTEM"]["llm_mode"] = selected_llm_mode
        save_configuration()

with tab_modelscope:
    st.subheader("ModelScope (è¯­éŸ³è¯†åˆ«) è®¾ç½®")
    if "MODELSCOPE" not in config:
        config.add_section("MODELSCOPE")

    modelscope_cache_path = st.text_input(
        "æ¨¡å‹ç¼“å­˜è·¯å¾„:",
        config.get(
            "MODELSCOPE",
            "MODELSCOPE_CACHE",
            fallback=os.path.join(os.getcwd(), "modelscope_cache"),
        ),
        help="ModelScopeä¸‹è½½å’ŒåŠ è½½æ¨¡å‹çš„æœ¬åœ°ç¼“å­˜ç›®å½•ã€‚",
    )
    output_dir_path = st.text_input(
        "è½¬å½•ç»“æœè¾“å‡ºè·¯å¾„:",
        config.get(
            "MODELSCOPE",
            "output_dir",
            fallback=os.path.join(os.getcwd(), "transcription_results"),
        ),
        help="ä¿å­˜éŸ³é¢‘è½¬å½•ç»“æœçš„æ ¹ç›®å½•ã€‚",
    )

    if st.button("ä¿å­˜ModelScopeé…ç½®", key="save_modelscope_settings", type="primary"):
        config["MODELSCOPE"]["MODELSCOPE_CACHE"] = modelscope_cache_path
        config["MODELSCOPE"]["output_dir"] = output_dir_path
        if save_configuration():
            if not os.path.exists(modelscope_cache_path):
                os.makedirs(modelscope_cache_path, exist_ok=True)
            if not os.path.exists(output_dir_path):
                os.makedirs(output_dir_path, exist_ok=True)


with tab_ollama:
    st.subheader("Ollama è®¾ç½®")
    if "OLLAMA" not in config:
        config.add_section("OLLAMA")

    ollama_base_url = st.text_input(
        "Ollama API åœ°å€ (Base URL):",
        config.get("OLLAMA", "base_url", fallback="http://localhost:11434"),
        help="ä¾‹å¦‚: http://localhost:11434",
    )

    ollama_model_list = []
    if ollama_base_url:
        try:
            # This part of fetching model list might need adjustment if ollama_scripts.get_ollama_config_values()
            # strictly reads from the saved file. For now, assume it can work or user saves then reloads.
            ollama_model_list = ollama_scripts.get_ollama_model_list()
        except Exception as e:
            st.warning(
                f"æ— æ³•ä» {ollama_base_url} è·å–Ollamaæ¨¡å‹åˆ—è¡¨: {e}. è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œä¸”åœ°å€æ­£ç¡®ã€‚"
            )

    current_ollama_model = config.get("OLLAMA", "model", fallback="")
    if ollama_model_list:
        selected_ollama_model = st.selectbox(
            "é€‰æ‹©Ollamaæ¨¡å‹:",
            ollama_model_list,
            index=(
                ollama_model_list.index(current_ollama_model)
                if current_ollama_model in ollama_model_list
                else 0
            ),
            help="ä»OllamaæœåŠ¡è·å–çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚",
        )
    else:
        selected_ollama_model = st.text_input(
            "æ‰‹åŠ¨è¾“å…¥Ollamaæ¨¡å‹åç§°:",
            current_ollama_model,
            help="å¦‚æœæ— æ³•è‡ªåŠ¨è·å–åˆ—è¡¨ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: llama2:latest)ã€‚",
        )

    ollama_max_tokens_ctx = st.number_input(
        "Ollama ä¸Šä¸‹æ–‡çª—å£å¤§å° (num_ctx):",
        min_value=512,
        max_value=128000, # Increased max context
        value=int(
            config.get("OLLAMA", "max_tokens", fallback="4096")
        ),
        step=256,
        help="æ¨¡å‹å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰ã€‚",
    )
    ollama_temperature = st.slider(
        "Ollama Temperature:",
        0.0,
        2.0,
        float(config.get("OLLAMA", "temperature", fallback="0.7")),
        step=0.05,
        help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒä½å€¼æ›´ä¿å®ˆï¼Œè¾ƒé«˜å€¼æ›´å…·åˆ›é€ æ€§ã€‚",
    )
    ollama_top_p = st.slider(
        "Ollama Top_p:",
        0.0,
        1.0,
        float(config.get("OLLAMA", "top_p", fallback="1.0")),
        step=0.05,
        help="æ ¸å¿ƒé‡‡æ ·å‚æ•°ã€‚æ¨¡å‹ä¼šè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°top_pçš„æœ€é«˜æ¦‚ç‡è¯æ±‡ã€‚",
    )

    if st.button("ä¿å­˜Ollamaé…ç½®", key="save_ollama_settings", type="primary"):
        if not ollama_base_url.strip():
            st.error("Ollama API åœ°å€ä¸èƒ½ä¸ºç©ºã€‚")
            st.stop()
        if not selected_ollama_model.strip():
            st.error("Ollama æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©ºã€‚")
            st.stop()

        config["OLLAMA"]["base_url"] = ollama_base_url
        config["OLLAMA"]["model"] = selected_ollama_model
        config["OLLAMA"]["max_tokens"] = str(ollama_max_tokens_ctx)
        config["OLLAMA"]["temperature"] = str(ollama_temperature)
        config["OLLAMA"]["top_p"] = str(ollama_top_p)
        if save_configuration():
            st.rerun()

with tab_online_model: # Changed variable name for the tab
    st.subheader("åœ¨çº¿æ¨¡å‹ (OpenAIå…¼å®¹) è®¾ç½®")
    st.caption(
        "ç®¡ç†é€šè¿‡OpenAIå…¼å®¹APIæ¥å£è®¿é—®çš„å„ç±»åœ¨çº¿å¤§æ¨¡å‹ã€‚è¯·å‡†ç¡®å¡«å†™æ¨¡å‹çš„APIåœ°å€ (Base URL)ã€APIå¯†é’¥ (API Key) å’Œæ¨¡å‹åç§° (Model Name)ã€‚"
    )
    # The [OPENAI] section in config.ini will store default model and parameters for this type
    if "OPENAI" not in config:
        config.add_section("OPENAI")

    @st.dialog("åœ¨çº¿æ¨¡å‹é…ç½®ç®¡ç†")
    def manage_online_models_dialog(): # Renamed dialog function
        st.markdown(
            """
        åœ¨è¿™é‡Œç®¡ç†æ‚¨çš„åœ¨çº¿æ¨¡å‹é…ç½®ã€‚æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å…¶ç‹¬ç«‹çš„APIåœ°å€ã€APIå¯†é’¥å’Œæ¨¡å‹åç§°ã€‚
        è¿™äº›ä¿¡æ¯å°†å­˜å‚¨åœ¨ `config/openai.json` æ–‡ä»¶ä¸­ã€‚
        **æ³¨æ„**ï¼šAPIå¯†é’¥åœ¨ç¼–è¾‘æ—¶å°†ä»¥æ˜æ–‡æ˜¾ç¤ºã€‚ç¡®ä¿æ­¤é…ç½®æ–‡ä»¶çš„å®‰å…¨ã€‚
        """
        )

        current_online_models = openai_scripts.get_openai_model_info()

        edited_models = st.data_editor(
            current_online_models,
            num_rows="dynamic",
            use_container_width=True,
            key="online_model_editor", # Changed key
            column_config={
                "model": st.column_config.TextColumn(
                    "æ¨¡å‹åç§° (ä¾‹å¦‚ gpt-4o, deepseek-chat)", required=True
                ),
                "base_url": st.column_config.TextColumn(
                    "API åœ°å€ (Base URL)", required=True
                ),
                "api_key": st.column_config.TextColumn(
                    "API å¯†é’¥ (API Key)",
                    help="APIå¯†é’¥å°†å­˜å‚¨åœ¨ config/openai.json ä¸­ã€‚",
                    required=True, # Making API key required for most online models
                ),
            },
            height=300,
        )

        col_save, col_cancel = st.columns(2)
        with col_save:
            if st.button(
                "âœ… ä¿å­˜æ¨¡å‹é…ç½®",
                type="primary",
                use_container_width=True,
                key="save_online_models_dialog", # Changed key
            ):
                try:
                    is_valid = True
                    for item in edited_models:
                        if (
                            not item.get("model", "").strip()
                            or not item.get("base_url", "").strip()
                            or not item.get("api_key", "").strip() # Check API key too
                        ):
                            st.error("æ¨¡å‹åç§°ã€APIåœ°å€å’ŒAPIå¯†é’¥å‡ä¸èƒ½ä¸ºç©ºã€‚è¯·å¡«å†™æ‰€æœ‰å¿…å¡«é¡¹ã€‚")
                            is_valid = False
                            break
                    
                    unique_model_names = {item.get("model", "").strip() for item in edited_models}
                    if len(unique_model_names) != len(edited_models):
                        st.error("æ¨¡å‹åç§°å¿…é¡»å”¯ä¸€ã€‚è¯·æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ¨¡å‹åç§°ã€‚")
                        is_valid = False


                    if is_valid:
                        if openai_scripts.update_openai_model_info(edited_models):
                            st.session_state.online_model_dialog_open = False
                            st.toast("åœ¨çº¿æ¨¡å‹é…ç½®å·²ä¿å­˜ï¼", icon="âœ…")
                            st.rerun()
                        else:
                            st.error("ä¿å­˜åœ¨çº¿æ¨¡å‹é…ç½®å¤±è´¥ã€‚")
                except ValueError as ve: # Catch validation errors from update_openai_model_info
                    st.error(f"é…ç½®é”™è¯¯: {ve}")
                except Exception as e:
                    st.error(f"ä¿å­˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        with col_cancel:
            if st.button(
                "âŒ å–æ¶ˆ", use_container_width=True, key="cancel_online_models_dialog" # Changed key
            ):
                st.session_state.online_model_dialog_open = False
                st.rerun()

    if st.button(
        "ğŸ› ï¸ ç®¡ç†åœ¨çº¿æ¨¡å‹åˆ—è¡¨",
        help="æ·»åŠ ã€ç¼–è¾‘æˆ–åˆ é™¤åœ¨çº¿æ¨¡å‹çš„é…ç½® (æ¨¡å‹åç§°, API Key, Base URL)ã€‚",
    ):
        st.session_state.online_model_dialog_open = True # Changed session state key

    if st.session_state.get("online_model_dialog_open", False): # Changed session state key
        manage_online_models_dialog()

    online_model_names_list = openai_scripts.get_openai_model_names()
    current_default_online_model = config.get("OPENAI", "model", fallback="")

    if online_model_names_list:
        selected_default_online_model = st.selectbox(
            "é€‰æ‹©é»˜è®¤åœ¨çº¿æ¨¡å‹:",
            online_model_names_list,
            index=(
                online_model_names_list.index(current_default_online_model)
                if current_default_online_model in online_model_names_list
                else 0
            ),
            help="é€‰æ‹©ä¸€ä¸ªåœ¨ä¸Šé¢â€œæ¨¡å‹ç®¡ç†â€ä¸­é…ç½®å¥½çš„æ¨¡å‹ä½œä¸ºé»˜è®¤ä½¿ç”¨ã€‚",
            key="online_default_model_selector", # Changed key
        )
    elif current_default_online_model: # List is empty, but a default was saved
        selected_default_online_model = st.text_input(
            "å½“å‰é»˜è®¤åœ¨çº¿æ¨¡å‹ (åˆ—è¡¨ä¸ºç©º):",
            current_default_online_model,
            disabled=True,
            help="è¯·é€šè¿‡â€œç®¡ç†åœ¨çº¿æ¨¡å‹åˆ—è¡¨â€æ·»åŠ æ¨¡å‹åå†é€‰æ‹©ã€‚"
        )
        st.warning("åœ¨çº¿æ¨¡å‹åˆ—è¡¨ä¸ºç©ºã€‚è¯·é€šè¿‡â€œç®¡ç†åœ¨çº¿æ¨¡å‹åˆ—è¡¨â€æ·»åŠ æ¨¡å‹é…ç½®ã€‚")
    else: # List is empty and no default saved
        selected_default_online_model = ""
        st.warning("æ²¡æœ‰å¯ç”¨çš„åœ¨çº¿æ¨¡å‹ã€‚è¯·é€šè¿‡â€œç®¡ç†åœ¨çº¿æ¨¡å‹åˆ—è¡¨â€æ·»åŠ å’Œé…ç½®æ¨¡å‹ã€‚")


    # Parameters for the default online model (still stored in [OPENAI] section of config.ini)
    openai_max_tokens = st.number_input(
        "é»˜è®¤Tokensä¸Šé™ (max_tokens):",
        min_value=50,
        max_value=128000, # Increased max
        value=int(config.get("OPENAI", "max_tokens", fallback="4096")),
        step=10,
        help="æ‰€é€‰é»˜è®¤åœ¨çº¿æ¨¡å‹å•æ¬¡è°ƒç”¨å¯ç”Ÿæˆçš„æœ€å¤§Tokenæ•°é‡ã€‚",
        key="online_model_max_tokens_input", # Changed key
    )
    openai_temperature = st.slider(
        "é»˜è®¤Temperature:",
        0.0,
        2.0,
        float(config.get("OPENAI", "temperature", fallback="0.7")),
        step=0.05,
        help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒä½å€¼ä½¿å…¶æ›´ä¿å®ˆï¼Œè¾ƒé«˜å€¼æ›´å…·åˆ›é€ æ€§ã€‚",
        key="online_model_temp_slider", # Changed key
    )
    openai_top_p = st.slider(
        "é»˜è®¤Top_p:",
        0.0,
        1.0,
        float(config.get("OPENAI", "top_p", fallback="1.0")),
        step=0.05,
        help="æ ¸å¿ƒé‡‡æ ·å‚æ•°ã€‚æ¨¡å‹ä¼šè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°top_pçš„æœ€é«˜æ¦‚ç‡è¯æ±‡ã€‚",
        key="online_model_top_p_slider", # Changed key
    )

    if st.button("ä¿å­˜åœ¨çº¿æ¨¡å‹é»˜è®¤è®¾ç½®", key="save_online_model_defaults", type="primary"): # Changed key
        if not selected_default_online_model and online_model_names_list:
            st.error("è¯·é€‰æ‹©ä¸€ä¸ªé»˜è®¤çš„åœ¨çº¿æ¨¡å‹ã€‚")
        elif not online_model_names_list and not selected_default_online_model:
            st.warning("æ²¡æœ‰é…ç½®ä»»ä½•åœ¨çº¿æ¨¡å‹ï¼Œæ— æ³•ä¿å­˜é»˜è®¤è®¾ç½®ã€‚è¯·å…ˆç®¡ç†æ¨¡å‹åˆ—è¡¨ã€‚")
        else:
            config["OPENAI"]["model"] = selected_default_online_model
            config["OPENAI"]["max_tokens"] = str(openai_max_tokens)
            config["OPENAI"]["temperature"] = str(openai_temperature)
            config["OPENAI"]["top_p"] = str(openai_top_p)
            save_configuration()