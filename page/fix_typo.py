import streamlit as st
import sys
import os

# Ensure the project root is in sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from scripts.utils import get_prompts_details, copy_text_to_clipboard, load_config_section, extract_and_clean_think_tags
from scripts.ollama_scripts import generate_ollama_completion
from scripts.openai_scripts import generate_openai_completion, get_openai_model_names # Stays openai_scripts


st.subheader("ä¿®æ­£æ–‡æœ¬")

with st.container(border=True):
    col1, col2 = st.columns(2)

    with col1:
        prompt_lists = get_prompts_details("fix_typo_prompt")
        
        fallback_default_prompt_content = "è¯·ä¿®æ­£ä»¥ä¸‹æ–‡æœ¬ä¸­çš„æ‹¼å†™å’Œè¯­æ³•é”™è¯¯ï¼š\n"
        
        if not prompt_lists:
            st.warning("æœªèƒ½åŠ è½½â€œä¿®æ­£æ–‡æœ¬â€æç¤ºè¯ã€‚è¯·æ£€æŸ¥`prompts.json`é…ç½®æ–‡ä»¶ã€‚")
            prompt_titles = []
            current_default_prompt_content = fallback_default_prompt_content
        else:
            prompt_titles = [prompt['title'] for prompt in prompt_lists]
        
        selected_prompt_title = st.selectbox(
            "é€‰æ‹©ä¿®æ­£æ¨¡æ¿:", 
            prompt_titles,
            index=0 if prompt_titles else None,
            disabled=not prompt_titles,
            key="fix_typo_prompt_selector"
        )
        
        if selected_prompt_title and prompt_lists:
            current_default_prompt_content = next(
                (p['content'] for p in prompt_lists if p['title'] == selected_prompt_title), 
                fallback_default_prompt_content
            )
        elif not prompt_lists:
            pass 
        else: 
             current_default_prompt_content = fallback_default_prompt_content
        
        with st.expander("æŸ¥çœ‹å’Œè°ƒæ•´æ¨¡æ¿", expanded=True):
            edited_prompt_content = st.text_area(
                "æ¨¡æ¿å†…å®¹:", 
                value=current_default_prompt_content, 
                height=250,
                key="fix_typo_edited_prompt"
            )

    with col2:
        if st.session_state.get('transcription_speaker_text'):
            use_transcription_text = st.radio(
                "ä½¿ç”¨ä¸Šä¸€é¡µçš„è½¬å½•ç»“æœï¼Ÿ",
                options=["æ˜¯", "å¦"],
                index=1,  # é»˜è®¤é€‰æ‹©â€œå¦â€
                horizontal=True,
                key="use_transcription_text"
            )
            if use_transcription_text == "æ˜¯":
                text_to_fix = st.text_area("å¾…ä¿®æ­£æ–‡æœ¬:", height=300, key="ft_text_input",value=st.session_state.transcription_speaker_text)
            else:
                text_to_fix = st.text_area("å¾…ä¿®æ­£æ–‡æœ¬:", height=300, key="ft_text_input")
        else:
            text_to_fix = st.text_area("å¾…ä¿®æ­£æ–‡æœ¬:", height=300, key="ft_text_input")


with st.container():
    fix_button = st.button("å¼€å§‹ä¿®æ­£", type="primary")
    
    st.markdown("---")
    st.write("#### ä¿®æ­£ç»“æœ")

    if 'ft_cleaned_text' not in st.session_state:
        st.session_state['ft_cleaned_text'] = ""
    if 'ft_thoughts' not in st.session_state:
        st.session_state['ft_thoughts'] = ""

    if fix_button and text_to_fix:
        if not edited_prompt_content.strip():
            st.error("æç¤ºè¯æ¨¡æ¿ä¸èƒ½ä¸ºç©ºã€‚")
        else:
            st.session_state['ft_cleaned_text'] = "" 
            st.session_state['ft_thoughts'] = ""

            with st.spinner('ä¿®æ­£ä¸­ï¼Œè¯·ç¨å€™...'):
                final_prompt_for_llm = edited_prompt_content + "\n" + text_to_fix
                
                full_raw_response = ""
                response_stream = None

                try:
                    system_config = load_config_section("SYSTEM")
                    llm_mode = system_config.get('llm_mode', 'Ollama') # 'Ollama' or 'OpenAI'

                    if llm_mode == 'Ollama':
                        response_stream = generate_ollama_completion(final_prompt_for_llm)
                    elif llm_mode == 'OpenAI': # This refers to using an OpenAI-compatible API
                        openai_config = load_config_section("OPENAI") # Reads [OPENAI] from config.ini
                        openai_model_name = openai_config.get('model') # Gets default model from [OPENAI]
                        if not openai_model_name:
                            st.error("é»˜è®¤åœ¨çº¿æ¨¡å‹æœªåœ¨config.iniä¸­é…ç½®ã€‚è¯·å…ˆåœ¨ è®¾ç½® > åœ¨çº¿æ¨¡å‹ é¡µé¢é…ç½®ã€‚")
                            st.stop()
                        # get_openai_model_names() fetches from openai.json
                        available_online_models = get_openai_model_names() 
                        if openai_model_name not in available_online_models:
                             st.warning(f"é…ç½®çš„é»˜è®¤åœ¨çº¿æ¨¡å‹ '{openai_model_name}' æœªåœ¨ 'config/openai.json' ä¸­æ‰¾åˆ°æˆ–å®šä¹‰ã€‚è¯·æ£€æŸ¥è®¾ç½®ã€‚")
                             # Potentially stop or allow proceeding if user confirms
                        response_stream = generate_openai_completion(final_prompt_for_llm, openai_model_name)
                    else:
                        st.error(f"ä¸æ”¯æŒçš„LLMæ¨¡å¼: {llm_mode}")
                        st.stop()
                    
                    for chunk in response_stream:
                        full_raw_response += chunk
                    
                    cleaned_text, thoughts = extract_and_clean_think_tags(full_raw_response)
                    
                    st.session_state['ft_cleaned_text'] = cleaned_text
                    st.session_state['ft_thoughts'] = thoughts
                    st.success("ä¿®æ­£å®Œæˆï¼")

                except ValueError as ve: 
                    st.error(f"é…ç½®é”™è¯¯: {ve}")
                except Exception as e:
                    st.error(f"ä¿®æ­£è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                    st.session_state['ft_cleaned_text'] = full_raw_response 
                    st.session_state['ft_thoughts'] = f"é”™è¯¯å‘ç”Ÿï¼Œæœªèƒ½è§£ææ€è€ƒè¿‡ç¨‹: {e}"
    
    if st.session_state.get('ft_cleaned_text'):
        st.markdown(st.session_state.ft_cleaned_text)
        st.button("å¤åˆ¶ä¿®æ­£ç»“æœ", on_click=copy_text_to_clipboard, args=(st.session_state.ft_cleaned_text,), key="copy_ft_cleaned_result")

    if st.session_state.get('ft_thoughts'):
        with st.expander("æŸ¥çœ‹æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ ğŸ¤”"):
            st.markdown(st.session_state.ft_thoughts)
            st.button("å¤åˆ¶æ€è€ƒè¿‡ç¨‹", on_click=copy_text_to_clipboard, args=(st.session_state.ft_thoughts,), key="copy_ft_thoughts_result")