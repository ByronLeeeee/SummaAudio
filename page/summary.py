import streamlit as st
import sys
import os

# Ensure the project root is in sys.path
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)

from scripts.ollama_scripts import generate_ollama_completion
from scripts.openai_scripts import (
    generate_openai_completion,
    get_openai_model_names,
)  # Stays openai_scripts
from scripts.utils import (
    get_prompts_details,
    copy_text_to_clipboard,
    load_config_section,
    extract_and_clean_think_tags,
)

st.subheader("âœï¸ æ–‡æœ¬å½’çº³ä¸æ‘˜è¦")

col_config, col_text_input = st.columns([1, 2])

with col_config:
    st.markdown("#### é…ç½®å½’çº³é€‰é¡¹")
    summary_type = st.radio(
        "é€‰æ‹©å½’çº³ç±»å‹:",
        ["æ‘˜è¦", "ä¼šè®®çºªè¦"],
        horizontal=True,
        key="summary_type_selector",
    )

    prompt_category = (
        "summary_prompt" if summary_type == "æ‘˜è¦" else "meeting_minutes_prompt"
    )
    prompt_lists = get_prompts_details(prompt_category)

    default_prompt_content_summary = f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä»½{summary_type}ï¼š\n"
    if not prompt_lists:
        st.warning(f"æœªèƒ½åŠ è½½â€œ{summary_type}â€æç¤ºè¯ã€‚è¯·æ£€æŸ¥`prompts.json`ã€‚")
        prompt_titles = []
    else:
        prompt_titles = [prompt["title"] for prompt in prompt_lists]

    selected_prompt_title = st.selectbox(
        f"é€‰æ‹©{summary_type}æ¨¡æ¿:",
        prompt_titles,
        index=0 if prompt_titles else None,
        disabled=not prompt_lists,
        key="summary_prompt_selector",
    )

    current_default_prompt_content_summary = default_prompt_content_summary
    if selected_prompt_title and prompt_lists:
        current_default_prompt_content_summary = next(
            (p["content"] for p in prompt_lists if p["title"] == selected_prompt_title),
            default_prompt_content_summary,
        )

    with st.expander("æŸ¥çœ‹å’Œè°ƒæ•´æ¨¡æ¿", expanded=False):
        edited_prompt_content = st.text_area(
            "æ¨¡æ¿å†…å®¹:",
            value=current_default_prompt_content_summary,
            height=150,
            key="summary_prompt_editor",
        )

    generate_button = st.button(
        f"å¼€å§‹ç”Ÿæˆ{summary_type}", type="primary", use_container_width=True
    )

with col_text_input:
    st.markdown("#### è¾“å…¥å¾…å¤„ç†æ–‡æœ¬")
    if st.session_state.get("ft_cleaned_text"):
        use_fix_typo_text = st.radio(
            "ä½¿ç”¨ä¿®æ­£ç»“æœï¼Ÿ",
            options=["æ˜¯", "å¦"],
            index=1,  # é»˜è®¤é€‰æ‹©â€œå¦â€
            horizontal=True,
            key="use_fix_typo_text",
        )
        if use_fix_typo_text == "æ˜¯":
            text_to_summarize = st.text_area(
                "å¾…å½’çº³æ–‡æœ¬:",
                height=400,
                key="summary_text_input",
                placeholder="åœ¨æ­¤å¤„ç²˜è´´æˆ–è¾“å…¥éœ€è¦å½’çº³çš„æ–‡æœ¬...",
                value=st.session_state.ft_cleaned_text,
            )
        else:
            text_to_summarize = st.text_area(
                "å¾…å½’çº³æ–‡æœ¬:",
                height=400,
                key="summary_text_input",
                placeholder="åœ¨æ­¤å¤„ç²˜è´´æˆ–è¾“å…¥éœ€è¦å½’çº³çš„æ–‡æœ¬...",
            )
    else:
        text_to_summarize = st.text_area(
            "å¾…å½’çº³æ–‡æœ¬:",
            height=400,
            key="summary_text_input",
            placeholder="åœ¨æ­¤å¤„ç²˜è´´æˆ–è¾“å…¥éœ€è¦å½’çº³çš„æ–‡æœ¬...",
        )

st.markdown("---")
st.subheader("ğŸš€ ç”Ÿæˆç»“æœ")

if "sm_cleaned_text" not in st.session_state:
    st.session_state.sm_cleaned_text = ""
if "sm_thoughts" not in st.session_state:
    st.session_state.sm_thoughts = ""


if generate_button and text_to_summarize:
    if not edited_prompt_content.strip():
        st.error("æç¤ºè¯æ¨¡æ¿ä¸èƒ½ä¸ºç©ºã€‚")
    else:
        st.session_state.sm_cleaned_text = ""
        st.session_state.sm_thoughts = ""

        with st.spinner(f"æ­£åœ¨ç”Ÿæˆ{summary_type}ï¼Œè¯·ç¨å€™..."):
            final_prompt_for_llm = edited_prompt_content + "\n" + text_to_summarize

            full_raw_response = ""
            response_stream = None
            try:
                system_config = load_config_section("SYSTEM")
                llm_mode = system_config.get(
                    "llm_mode", "Ollama"
                )  # 'Ollama' or 'OpenAI'

                if llm_mode == "Ollama":
                    response_stream = generate_ollama_completion(final_prompt_for_llm)
                elif (
                    llm_mode == "OpenAI"
                ):  # This refers to using an OpenAI-compatible API
                    openai_config = load_config_section(
                        "OPENAI"
                    )  # Reads [OPENAI] from config.ini
                    openai_model_name = openai_config.get(
                        "model"
                    )  # Gets default model from [OPENAI]
                    if not openai_model_name:
                        st.error(
                            "é»˜è®¤åœ¨çº¿æ¨¡å‹æœªåœ¨config.iniä¸­é…ç½®ã€‚è¯·å…ˆåœ¨ è®¾ç½® > åœ¨çº¿æ¨¡å‹ é¡µé¢é…ç½®ã€‚"
                        )
                        st.stop()
                    # get_openai_model_names() fetches from openai.json
                    available_online_models = get_openai_model_names()
                    if openai_model_name not in available_online_models:
                        st.warning(
                            f"é…ç½®çš„é»˜è®¤åœ¨çº¿æ¨¡å‹ '{openai_model_name}' æœªåœ¨ 'config/openai.json' ä¸­æ‰¾åˆ°æˆ–å®šä¹‰ã€‚è¯·æ£€æŸ¥è®¾ç½®ã€‚"
                        )
                    response_stream = generate_openai_completion(
                        final_prompt_for_llm, openai_model_name
                    )
                else:
                    st.error(f"ä¸æ”¯æŒçš„LLMæ¨¡å¼: {llm_mode}")
                    st.stop()

                for chunk in response_stream:
                    full_raw_response += chunk

                cleaned_text, thoughts = extract_and_clean_think_tags(full_raw_response)

                st.session_state["sm_cleaned_text"] = cleaned_text
                st.session_state["sm_thoughts"] = thoughts
                st.success(f"{summary_type}ç”Ÿæˆå®Œæˆï¼")

            except ValueError as ve:
                st.error(f"é…ç½®é”™è¯¯: {ve}")
            except Exception as e:
                st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                st.session_state["sm_cleaned_text"] = full_raw_response
                st.session_state["sm_thoughts"] = f"é”™è¯¯å‘ç”Ÿï¼Œæœªèƒ½è§£ææ€è€ƒè¿‡ç¨‹: {e}"
elif generate_button and not text_to_summarize:
    st.warning("è¯·è¾“å…¥å¾…å½’çº³çš„æ–‡æœ¬ã€‚")

if st.session_state.get("sm_thoughts"):
    with st.expander("æŸ¥çœ‹æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ ğŸ¤”"):
        st.markdown(st.session_state.sm_thoughts)
        st.button(
            "å¤åˆ¶æ€è€ƒè¿‡ç¨‹",
            on_click=copy_text_to_clipboard,
            args=(st.session_state.sm_thoughts,),
            key="copy_summary_thoughts_result",
        )

if st.session_state.get("sm_cleaned_text"):
    st.markdown(st.session_state.sm_cleaned_text)
    st.button(
        "å¤åˆ¶ç»“æœåˆ°å‰ªè´´æ¿",
        on_click=copy_text_to_clipboard,
        args=(st.session_state.sm_cleaned_text,),
        key="copy_summary_cleaned_result",
    )
