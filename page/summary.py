import streamlit as st
import sys
import os

# Ensure the project root is in sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from scripts.ollama_scripts import generate_ollama_completion
from scripts.openai_scripts import generate_openai_completion, get_openai_model_names
from scripts.utils import get_prompts_details, copy_text_to_clipboard, load_config_section

st.subheader("âœï¸ æ–‡æœ¬å½’çº³ä¸æ‘˜è¦")

col_config, col_text_input = st.columns([1, 2]) # Adjusted column ratio

with col_config:
    st.markdown("#### é…ç½®å½’çº³é€‰é¡¹")
    summary_type = st.radio(
        "é€‰æ‹©å½’çº³ç±»å‹:", 
        ["æ‘˜è¦", "ä¼šè®®çºªè¦"], 
        horizontal=True,
        key="summary_type_selector"
    )

    prompt_category = "summary_prompt" if summary_type == "æ‘˜è¦" else "meeting_minutes_prompt"
    prompt_lists = get_prompts_details(prompt_category)

    if not prompt_lists:
        st.warning(f"æœªèƒ½åŠ è½½â€œ{summary_type}â€æç¤ºè¯ã€‚è¯·æ£€æŸ¥`prompts.json`ã€‚")
        prompt_titles = []
        default_prompt_content = f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä»½{summary_type}ï¼š\n"
    else:
        prompt_titles = [prompt['title'] for prompt in prompt_lists]
    
    selected_prompt_title = st.selectbox(
        f"é€‰æ‹©{summary_type}æ¨¡æ¿:", 
        prompt_titles,
        index=0 if prompt_titles else None,
        disabled=not prompt_titles,
        key="summary_prompt_selector"
    )
    
    if selected_prompt_title:
        default_prompt_content = next(
            (p['content'] for p in prompt_lists if p['title'] == selected_prompt_title), 
            f"è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä»½{summary_type}ï¼š\n"
        )

    with st.expander("æŸ¥çœ‹å’Œè°ƒæ•´æ¨¡æ¿", expanded=False):
        edited_prompt_content = st.text_area(
            "æ¨¡æ¿å†…å®¹:", 
            value=default_prompt_content, 
            height=150,
            key="summary_prompt_editor"
        )
    
    generate_button = st.button(f"å¼€å§‹ç”Ÿæˆ{summary_type}", type="primary", use_container_width=True)

with col_text_input:
    st.markdown("#### è¾“å…¥å¾…å¤„ç†æ–‡æœ¬")
    text_to_summarize = st.text_area(
        "å¾…å½’çº³æ–‡æœ¬:", 
        height=400,  # Increased height
        key="summary_text_input",
        placeholder="åœ¨æ­¤å¤„ç²˜è´´æˆ–è¾“å…¥éœ€è¦å½’çº³çš„æ–‡æœ¬..."
    )

st.markdown("---")
st.subheader("ğŸš€ ç”Ÿæˆç»“æœ")

if 'sm_result_text' not in st.session_state:
    st.session_state.sm_result_text = ""

if generate_button and text_to_summarize:
    if not edited_prompt_content.strip():
        st.error("æç¤ºè¯æ¨¡æ¿ä¸èƒ½ä¸ºç©ºã€‚")
    else:
        with st.spinner(f"æ­£åœ¨ç”Ÿæˆ{summary_type}ï¼Œè¯·ç¨å€™..."):
            final_prompt_for_llm = edited_prompt_content + "\n" + text_to_summarize
            
            try:
                system_config = load_config_section("SYSTEM")
                llm_mode = system_config.get('llm_mode', 'Ollama')

                if llm_mode == 'Ollama':
                    response_stream = generate_ollama_completion(final_prompt_for_llm)
                elif llm_mode == 'OpenAI':
                    openai_config = load_config_section("OPENAI")
                    openai_model_name = openai_config.get('model')
                    if not openai_model_name:
                        st.error("OpenAIæ¨¡å‹æœªåœ¨config.iniä¸­é…ç½®ã€‚è¯·å…ˆåœ¨è®¾ç½®é¡µé¢é…ç½®ã€‚")
                        st.stop()
                    available_openai_models = get_openai_model_names()
                    if openai_model_name not in available_openai_models:
                         st.warning(f"é…ç½®çš„OpenAIæ¨¡å‹ '{openai_model_name}' å¯èƒ½ä¸å¯ç”¨æˆ–æœªåœ¨ `openai.json` ä¸­å®šä¹‰ã€‚")
                    response_stream = generate_openai_completion(final_prompt_for_llm, openai_model_name)
                else:
                    st.error(f"ä¸æ”¯æŒçš„LLMæ¨¡å¼: {llm_mode}")
                    st.stop()

                response_placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    full_response += chunk
                    response_placeholder.markdown(full_response) 
                
                st.session_state['sm_result_text'] = full_response
                st.success(f"{summary_type}ç”Ÿæˆå®Œæˆï¼")

            except ValueError as ve:
                st.error(f"é…ç½®é”™è¯¯: {ve}")
            except Exception as e:
                st.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                st.session_state['sm_result_text'] = "" # Clear on error
elif generate_button and not text_to_summarize:
    st.warning("è¯·è¾“å…¥å¾…å½’çº³çš„æ–‡æœ¬ã€‚")


# Display result from session state
if st.session_state.get('sm_result_text'):
    st.text_area("ç»“æœé¢„è§ˆ:", st.session_state.sm_result_text, height=300, disabled=True, key="summary_result_display")
    st.button("å¤åˆ¶ç»“æœåˆ°å‰ªè´´æ¿", on_click=copy_text_to_clipboard, args=(st.session_state.sm_result_text,), key="copy_summary_result")