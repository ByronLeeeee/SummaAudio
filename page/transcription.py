import re
import streamlit as st
import sys
import os

# Ensure the project root is in sys.path
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)

from scripts.modelscope_scripts import (
    run_modelscope_recognition,
    organize_recognition_results,
    save_transcription_results,
    display_modelscope_model_selector,  # Renamed and behavior changed
)
from scripts.utils import setup_logger, copy_text_to_clipboard

logger = setup_logger("TranscriptionPage")
CACHE_DIR = "cache"  # Define cache directory

# Initialize session state keys
SESSION_STATE_KEYS_TRANSCRIPTION = {
    "transcription_audio_filename": None,
    "transcription_full_text": "",
    "transcription_speaker_text": "",
}
for key, default_value in SESSION_STATE_KEYS_TRANSCRIPTION.items():
    if key not in st.session_state:
        st.session_state[key] = default_value


def cleanup_transcription_state():
    """æ¸…ç©ºè½¬å½•é¡µé¢çš„ä¼šè¯çŠ¶æ€ï¼ˆé™¤æ–‡ä»¶åå¤–ï¼‰ã€‚"""
    for key in SESSION_STATE_KEYS_TRANSCRIPTION:
        if key != "transcription_audio_filename":
            st.session_state[key] = SESSION_STATE_KEYS_TRANSCRIPTION[key]


@st.dialog("èŠå¤©æ¨¡å¼é¢„è§ˆ")  # Use experimental_dialog for Streamlit < 1.30
def display_chat_preview(speaker_separated_text: str):
    """
    ä»¥èŠå¤©æ¶ˆæ¯çš„å½¢å¼æ˜¾ç¤ºåŒºåˆ†è¯´è¯äººçš„æ–‡æœ¬ã€‚

    :param speaker_separated_text: str, åŒ…å«è¯´è¯äººæ ‡è®°çš„æ–‡æœ¬ã€‚
    """
    st.markdown("#### å¯¹è¯é¢„è§ˆ")
    if not speaker_separated_text.strip():
        st.info("æ— å†…å®¹å¯é¢„è§ˆã€‚")
        return

    # Regex to capture "è¯´è¯äººX: " and the subsequent text until the next speaker or end of string
    # Using re.DOTALL to make '.' match newlines as well
    segments = re.findall(
        r"(è¯´è¯äºº\d+):\s*(.*?)(?=\n\nè¯´è¯äºº\d+:|$)", speaker_separated_text, re.DOTALL
    )

    if not segments:  # Fallback if regex doesn't match expected format well
        st.warning("æ— æ³•æŒ‰èŠå¤©æ ¼å¼è§£ææ–‡æœ¬ï¼Œå°†æ˜¾ç¤ºåŸå§‹åˆ†æ®µæ–‡æœ¬ã€‚")
        st.text(speaker_separated_text)
        return

    for i, segment_match in enumerate(segments):
        speaker_tag = segment_match[0]  # e.g., "è¯´è¯äºº1"
        content = segment_match[1].strip()

        # Use a simple alternating avatar or derive from speaker_tag if desired
        avatar_icon = (
            "ğŸ§‘â€ğŸ’»" if (int(speaker_tag.replace("è¯´è¯äºº", "")) % 2 == 0) else "ğŸ‘¤"
        )

        with st.chat_message(name=speaker_tag, avatar=avatar_icon):
            st.markdown(
                content
            )  # Use markdown for better text rendering (e.g., newlines)


st.header("ğŸ¤ éŸ³é¢‘è½¬å½• (ModelScope)")
st.caption("ä½¿ç”¨ModelScopeæ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚")

# Sidebar for model selection
with st.sidebar:
    st.title("âš™ï¸ æ¨¡å‹é…ç½®")
    st.info("å»ºè®®ä½¿ç”¨é»˜è®¤æ¨¡å‹ç»„åˆã€‚æ›´æ”¹æ¨¡å‹å¯èƒ½å¯¼è‡´é¢„æ–™ä¹‹å¤–çš„è¡Œä¸ºæˆ–é”™è¯¯ã€‚")
    with st.expander("é€‰æ‹©è½¬å½•æ¨¡å‹", expanded=True):
        # This function now directly returns the selected model_id and revision_str
        (model_id, model_rev, vad_id, vad_rev, punc_id, punc_rev, spk_id, spk_rev) = (
            display_modelscope_model_selector()
        )

# Main area for file upload and results
uploaded_audio_file = st.file_uploader(
    "é€‰æ‹©æˆ–æ‹–æ”¾éŸ³é¢‘æ–‡ä»¶ (WAV, MP3, FLAC, M4A)",
    type=["wav", "mp3", "flac", "m4a"],
    key="transcription_audio_uploader",
)

if uploaded_audio_file:
    # If a new file is uploaded, clear previous results
    if st.session_state.get("transcription_audio_filename") != uploaded_audio_file.name:
        cleanup_transcription_state()
        st.session_state["transcription_audio_filename"] = uploaded_audio_file.name
        logger.info(f"New audio file for transcription: {uploaded_audio_file.name}")

    st.subheader("ğŸ”Š éŸ³é¢‘é¢„è§ˆ")
    st.audio(uploaded_audio_file, format=uploaded_audio_file.type)

    if st.button("â–¶ï¸ å¼€å§‹è¯†åˆ«", type="primary", use_container_width=True):
        if not model_id:  # Check if model selection from sidebar was successful
            st.error("ä¸»è½¬å½•æ¨¡å‹æœªé€‰æ‹©æˆ–åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é…ç½®ã€‚")
        else:
            cleanup_transcription_state()  # Clear previous results for this file
            st.session_state["transcription_audio_filename"] = (
                uploaded_audio_file.name
            )  # Keep filename

            # Ensure cache directory exists
            if not os.path.exists(CACHE_DIR):
                os.makedirs(CACHE_DIR)

            audio_file_path = os.path.join(CACHE_DIR, uploaded_audio_file.name)
            with open(audio_file_path, "wb") as f:
                f.write(uploaded_audio_file.getbuffer())

            with st.spinner("è¯†åˆ«ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…..."):
                try:
                    raw_result = run_modelscope_recognition(
                        audio_input_path=audio_file_path,
                        model_id=model_id,
                        model_revision=model_rev,
                        vad_model_id=vad_id,
                        vad_model_revision=vad_rev,
                        punc_model_id=punc_id,
                        punc_model_revision=punc_rev,
                        spk_model_id=spk_id,
                        spk_model_revision=spk_rev,
                    )

                    (
                        st.session_state.transcription_full_text,
                        st.session_state.transcription_speaker_text,
                    ) = organize_recognition_results(raw_result)

                    if (
                        not st.session_state.transcription_full_text
                        and not st.session_state.transcription_speaker_text
                    ):
                        st.warning("è¯†åˆ«ç»“æœä¸ºç©ºã€‚è¯·æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æˆ–æ¨¡å‹é…ç½®ã€‚")
                    else:
                        st.success("éŸ³é¢‘è¯†åˆ«å®Œæˆï¼")
                        # Save results automatically
                        filename_base = os.path.splitext(uploaded_audio_file.name)[0]
                        if save_transcription_results(
                            st.session_state.transcription_full_text,
                            st.session_state.transcription_speaker_text,
                            filename_base,
                            mode="normal",  # 'normal' for transcription page context
                        ):
                            st.toast(f"ç»“æœå·²ä¿å­˜åˆ° '{filename_base}' æ–‡ä»¶å¤¹ã€‚")
                        else:
                            st.error("ä¿å­˜è¯†åˆ«ç»“æœå¤±è´¥ã€‚")

                except Exception as e:
                    logger.error(f"Transcription process error: {e}", exc_info=True)
                    st.error(f"è¯†åˆ«è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                finally:
                    # Clean up cached audio file
                    if os.path.exists(audio_file_path):
                        try:
                            os.remove(audio_file_path)
                            logger.info(f"Cached audio file removed: {audio_file_path}")
                        except OSError as e:
                            logger.error(
                                f"Error removing cached file {audio_file_path}: {e}"
                            )
            st.rerun()  # Rerun to display new results from session state

# Display results if available in session state
if st.session_state.get("transcription_full_text") or st.session_state.get(
    "transcription_speaker_text"
):
    st.markdown("---")
    st.subheader("ğŸ“‹ è¯†åˆ«ç»“æœ")

    col1, col2 = st.columns(2)
    with col1:
        with st.expander("ğŸ“„ å®Œæ•´è¯†åˆ«ç»“æœ (æ— è¯´è¯äººåŒºåˆ†)", expanded=True):
            if st.session_state.transcription_full_text:
                st.text_area(
                    "å…¨æ–‡:",
                    st.session_state.transcription_full_text,
                    height=300,
                    disabled=True,
                    key="full_text_display",
                )
                st.button(
                    "å¤åˆ¶å…¨æ–‡",
                    on_click=copy_text_to_clipboard,
                    args=(st.session_state.transcription_full_text,),
                    key="copy_full_text",
                )
            else:
                st.info("æ— å®Œæ•´è¯†åˆ«ç»“æœã€‚")

    with col2:
        with st.expander("ğŸ—£ï¸ å¸¦è¯´è¯äººè¯†åˆ«ç»“æœ", expanded=True):
            if st.session_state.transcription_speaker_text:
                st.text_area(
                    "åˆ†æ®µæ–‡æœ¬:",
                    st.session_state.transcription_speaker_text,
                    height=300,
                    disabled=True,
                    key="speaker_text_display",
                )

                button_col_copy, button_col_chat = st.columns(2)
                with button_col_copy:
                    st.button(
                        "å¤åˆ¶åˆ†æ®µæ–‡æœ¬",
                        on_click=copy_text_to_clipboard,
                        args=(st.session_state.transcription_speaker_text,),
                        key="copy_speaker_text",
                        use_container_width=True,
                    )
                with button_col_chat:
                    if st.button(
                        "ğŸ’¬ èŠå¤©æ¨¡å¼é¢„è§ˆ",
                        key="chat_mode_button",
                        use_container_width=True,
                    ):
                        display_chat_preview(
                            st.session_state.transcription_speaker_text
                        )
            else:
                st.info("æ— è¯´è¯äººè¯†åˆ«ç»“æœ (å¯èƒ½æ¨¡å‹ä¸æ”¯æŒæˆ–æœªå¯ç”¨)ã€‚")
